# π“¦ Models ν΄λ”

νμΈνλ‹λ LoRA λ¨λΈμ„ μ €μ¥ν•λ” ν΄λ”μ…λ‹λ‹¤.

---

## π“ ν΄λ” κµ¬μ΅°

```
models/
β””β”€β”€ ems-lora-checkpoint/       β† checkpoint-500 ν΄λ”λ¥Ό μ—¬κΈ°μ— λ³µμ‚¬
    β”β”€β”€ adapter_config.json
    β”β”€β”€ adapter_model.safetensors
    β”β”€β”€ tokenizer.json
    β”β”€β”€ tokenizer_config.json
    β”β”€β”€ special_tokens_map.json
    β”β”€β”€ vocab.json
    β””β”€β”€ merges.txt
```

### 3οΈβƒ£ ν•„μ”ν• νμΌ

β… adapter_config.json          (LoRA μ„¤μ •)
β… adapter_model.safetensors    (ν•™μµλ κ°€μ¤‘μΉ - κ°€μ¥ μ¤‘μ”!)
β… tokenizer.json               (ν† ν¬λ‚μ΄μ €)
β… tokenizer_config.json        (ν† ν¬λ‚μ΄μ € μ„¤μ •)
β… special_tokens_map.json      (νΉμ ν† ν°)
β… vocab.json                   (μ–΄ν)
β… merges.txt                   (BPE λ³‘ν•©)
---

## π”§ ν™κ²½ λ³€μ μ„¤μ •

`config/.env` νμΌμ— μ¶”κ°€:
```bash
# LoRA μ–΄λ‘ν„° κ²½λ΅
LORA_ADAPTER_PATH=./models/ems-lora-checkpoint

# λ² μ΄μ¤ λ¨λΈ
BASE_MODEL_NAME=Qwen/Qwen2.5-3B-Instruct
```

---

## π”„ λ¨λΈ λ²„μ „ κ΄€λ¦¬

μƒλ΅μ΄ νμΈνλ‹ λ¨λΈλ΅ κµμ²΄:

```bash
# 1. μƒ μ²΄ν¬ν¬μΈνΈ λ³µμ‚¬
cp -r checkpoint-600 models/ems-lora-checkpoint-v2

# 2. APIλ΅ λ¨λΈ κµμ²΄
curl -X POST http://localhost:8000/stt-to-json/reload-model?adapter_path=./models/ems-lora-checkpoint-v2

# λλ” ν™κ²½ λ³€μ μμ • ν›„ μ„λ²„ μ¬μ‹μ‘
LORA_ADAPTER_PATH=./models/ems-lora-checkpoint-v2
```

---

## π“ λ¨λΈ ν¬κΈ° μμƒ

- **adapter_model.safetensors**: μ•½ 50-100MB
- **tokenizer νμΌλ“¤**: μ•½ 5MB
- **μ΄ν•©**: μ•½ 55-105MB

---

## π’΅ ν

### μ—¬λ¬ λ²„μ „ κ΄€λ¦¬
```
models/
β”β”€β”€ ems-lora-checkpoint-v1/    (μµμ΄ λ²„μ „)
β”β”€β”€ ems-lora-checkpoint-v2/    (κ°μ„  λ²„μ „)
β””β”€β”€ ems-lora-checkpoint-best/  (μµκ³  μ„±λ¥)
```

### Gitμ—μ„ μ μ™Έ
`.gitignore`μ— μ¶”κ°€:
```
models/*.safetensors
models/ems-lora-*/
```

---

**μ¤€λΉ„λλ©΄ μ„λ²„λ¥Ό μ‹μ‘ν•μ„Έμ”!** π€


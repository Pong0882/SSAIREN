"""
STT â†’ JSON ì¶”ì¶œ ë¼ìš°í„°
íŒŒì¸íŠœë‹ëœ LoRA LLMì„ ì‚¬ìš©í•œ ëŒ€í™” í…ìŠ¤íŠ¸ â†’ JSON ì¶”ì¶œ
"""
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel, Field
from typing import Dict, Any
from services.lora_llm_service import get_lora_llm_service

router = APIRouter(prefix="/stt-to-json", tags=["STT to JSON"])

class ConversationToJsonRequest(BaseModel):
    """ëŒ€í™” í…ìŠ¤íŠ¸ â†’ JSON ìš”ì²­"""
    conversation: str = Field(..., description="ëŒ€í™” í…ìŠ¤íŠ¸ (STT ê²°ê³¼ ë˜ëŠ” ì§ì ‘ ì…ë ¥)")
    max_new_tokens: int = Field(700, ge=400, le=1500, description="ìµœëŒ€ ìƒì„± í† í°")
    temperature: float = Field(0.1, ge=0.0, le=1.0, description="ìƒ˜í”Œë§ ì˜¨ë„")

@router.post("/text-to-json")
async def text_to_json(request: ConversationToJsonRequest):
    """
    ëŒ€í™” í…ìŠ¤íŠ¸ â†’ JSON ì¶”ì¶œ (STT ê²°ê³¼ ì§ì ‘ ì…ë ¥)
    
    ## ìš©ë„
    - STTê°€ ì´ë¯¸ ì™„ë£Œëœ ê²½ìš°
    - í…ŒìŠ¤íŠ¸ìš© ëŒ€í™” ì§ì ‘ ì…ë ¥
    """
    try:
        print(f"ğŸ¤– JSON ì¶”ì¶œ ì‹œì‘ (ëŒ€í™” ê¸¸ì´: {len(request.conversation)}ì)")
        
        llm_service = get_lora_llm_service()
        result = llm_service.extract_json_from_conversation(
            conversation=request.conversation,
            max_tokens=request.max_new_tokens,
            temperature=request.temperature
        )
        
        if result["success"]:
            print(f"âœ… JSON ì¶”ì¶œ ì™„ë£Œ")
            return {
                "success": True,
                "extracted_json": result["json"],
                "raw_output": result["raw_text"]
            }
        else:
            raise HTTPException(
                status_code=500,
                detail=f"JSON íŒŒì‹± ì‹¤íŒ¨: {result['error']}"
            )
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")

@router.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    try:
        llm_service = get_lora_llm_service()
        return {
            "status": "healthy",
            "model_loaded": llm_service._model is not None,
            "device": "CPU"  # llama-cpp-pythonì€ CPU ì „ìš©
        }
    except Exception as e:
        raise HTTPException(status_code=503, detail=f"ì„œë¹„ìŠ¤ ë¶ˆê°€: {str(e)}")


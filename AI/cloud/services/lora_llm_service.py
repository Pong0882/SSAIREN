"""
íŒŒì¸íŠœë‹ëœ LoRA ëª¨ë¸ì„ ì‚¬ìš©í•œ LLM ì„œë¹„ìŠ¤
STT ëŒ€ë³¸ â†’ JSON ì¶”ì¶œ

ëª¨ë¸ êµì²´ê°€ ì‰½ë„ë¡ ì„¤ê³„ë¨
"""
import os
import json
import torch
from typing import Optional, Dict, Any
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

class LoRALLMService:
    """íŒŒì¸íŠœë‹ëœ LoRA ëª¨ë¸ ì„œë¹„ìŠ¤ (Singleton)"""
    
    _instance: Optional['LoRALLMService'] = None
    _model = None
    _tokenizer = None
    _device = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        """ëª¨ë¸ ì´ˆê¸°í™” (ìµœì´ˆ 1íšŒë§Œ)"""
        if self._model is None:
            self._load_model()
    
    def _load_model(self):
        """LoRA ëª¨ë¸ ë¡œë“œ"""
        # í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê²½ë¡œ ì½ê¸°
        base_model_name = os.getenv("BASE_MODEL_NAME", "Qwen/Qwen2.5-3B-Instruct")
        adapter_path = os.getenv("LORA_ADAPTER_PATH", "./models/ems-lora-checkpoint")
        
        print(f"ğŸ”„ ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë”©: {base_model_name}")
        print(f"ğŸ”„ LoRA ì–´ëŒ‘í„° ë¡œë”©: {adapter_path}")
        
        # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        self._device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ğŸ’» ë””ë°”ì´ìŠ¤: {self._device}")
        
        try:
            # í† í¬ë‚˜ì´ì € ë¡œë“œ
            self._tokenizer = AutoTokenizer.from_pretrained(base_model_name)
            
            # ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
            self._model = AutoModelForCausalLM.from_pretrained(
                base_model_name,
                torch_dtype=torch.float16 if self._device == "cuda" else torch.float32,
                device_map="auto" if self._device == "cuda" else None,
                low_cpu_mem_usage=True
            )
            
            # LoRA ì–´ëŒ‘í„° ì ìš©
            if os.path.exists(adapter_path):
                print(f"âœ… LoRA ì–´ëŒ‘í„° ë°œê²¬: {adapter_path}")
                self._model = PeftModel.from_pretrained(self._model, adapter_path)
                print("âœ… LoRA ì–´ëŒ‘í„° ì ìš© ì™„ë£Œ!")
            else:
                print(f"âš ï¸  LoRA ì–´ëŒ‘í„° ì—†ìŒ, ë² ì´ìŠ¤ ëª¨ë¸ë§Œ ì‚¬ìš©: {adapter_path}")
            
            # ì¶”ë¡  ëª¨ë“œ
            self._model.eval()
            
            if self._device == "cpu":
                self._model = self._model.to("cpu")
            
            print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            raise
    
    def extract_json_from_conversation(
        self,
        conversation: str,
        max_new_tokens: int = 700,
        temperature: float = 0.1,
        top_p: float = 0.9
    ) -> Dict[str, Any]:
        """
        ëŒ€í™”ì—ì„œ JSON ì¶”ì¶œ
        
        Args:
            conversation: STTë¡œ ë³€í™˜ëœ ëŒ€í™” í…ìŠ¤íŠ¸
            max_new_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜
            temperature: ìƒ˜í”Œë§ ì˜¨ë„ (ë‚®ì„ìˆ˜ë¡ ê²°ì •ì )
            top_p: nucleus sampling
            
        Returns:
            ì¶”ì¶œëœ JSON (dict)
        """
        if self._model is None or self._tokenizer is None:
            raise RuntimeError("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„± (íŒŒì¸íŠœë‹ëœ í˜•ì‹ì— ë§ì¶¤)
        prompt = f"""ì•„ë˜ëŠ” ì‘ê¸‰êµ¬ì¡°ì‚¬ì™€ í™˜ìì˜ ëŒ€í™”ì…ë‹ˆë‹¤.
ì´ ëŒ€í™”ë¥¼ ì½ê³ , GBNF ìŠ¤í‚¤ë§ˆì— ë§ëŠ” JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”.
- ì„¤ëª… ë¬¸ì¥ ì“°ì§€ ë§ ê²ƒ
- ë§ˆí¬ë‹¤ìš´ ì½”ë“œë¸”ë¡( ``` ) ì“°ì§€ ë§ ê²ƒ
- ìˆœìˆ˜ JSONë§Œ ì¶œë ¥í•  ê²ƒ

{conversation}

### ì‘ë‹µ:
"""
        
        # í† í¬ë‚˜ì´ì§•
        inputs = self._tokenizer(prompt, return_tensors="pt")
        
        if self._device == "cuda":
            inputs = {k: v.to("cuda") for k, v in inputs.items()}
        
        # ìƒì„±
        with torch.no_grad():
            outputs = self._model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                top_p=top_p,
                do_sample=False,  # ê²°ì •ì  ìƒì„±
                pad_token_id=self._tokenizer.pad_token_id,
                eos_token_id=self._tokenizer.eos_token_id
            )
        
        # ë””ì½”ë”©
        decoded = self._tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # ì‘ë‹µ ë¶€ë¶„ë§Œ ì¶”ì¶œ
        generated_part = decoded.split("### ì‘ë‹µ:")[-1].strip()
        
        # ì½”ë“œë¸”ë¡ ì œê±° (``` ì œê±°)
        if "```" in generated_part:
            # ì²« ë²ˆì§¸ ```ì™€ ë§ˆì§€ë§‰ ``` ì‚¬ì´ì˜ ë‚´ìš© ì¶”ì¶œ
            start = generated_part.find("```")
            end = generated_part.rfind("```")
            if start != -1 and end != -1 and start < end:
                generated_part = generated_part[start + 3:end].strip()
                # jsonì´ë¼ëŠ” ì–¸ì–´ íƒœê·¸ ì œê±°
                if generated_part.startswith("json"):
                    generated_part = generated_part[4:].strip()
        
        # JSON íŒŒì‹±
        try:
            parsed_json = json.loads(generated_part)
            return {
                "success": True,
                "json": parsed_json,
                "raw_text": generated_part
            }
        except json.JSONDecodeError as e:
            # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë¬¸ ë°˜í™˜
            return {
                "success": False,
                "error": str(e),
                "raw_text": generated_part
            }
    
    def reload_model(self, adapter_path: str):
        """
        ìƒˆë¡œìš´ ì–´ëŒ‘í„°ë¡œ ëª¨ë¸ ì¬ë¡œë“œ
        (í•™ìŠµ í›„ ëª¨ë¸ êµì²´ ì‹œ ì‚¬ìš©)
        
        Args:
            adapter_path: ìƒˆë¡œìš´ LoRA ì–´ëŒ‘í„° ê²½ë¡œ
        """
        print(f"ğŸ”„ ëª¨ë¸ ì¬ë¡œë“œ ì‹œì‘: {adapter_path}")
        
        # ê¸°ì¡´ ëª¨ë¸ ë©”ëª¨ë¦¬ í•´ì œ
        if self._model is not None:
            del self._model
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        
        # ìƒˆ ê²½ë¡œë¡œ í™˜ê²½ ë³€ìˆ˜ ì—…ë°ì´íŠ¸
        os.environ["LORA_ADAPTER_PATH"] = adapter_path
        
        # ëª¨ë¸ ì¬ë¡œë“œ
        self._model = None
        self._load_model()
        
        print("âœ… ëª¨ë¸ ì¬ë¡œë“œ ì™„ë£Œ!")


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
_lora_llm_service: Optional[LoRALLMService] = None

def get_lora_llm_service() -> LoRALLMService:
    """LoRA LLM ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (Singleton)"""
    global _lora_llm_service
    if _lora_llm_service is None:
        _lora_llm_service = LoRALLMService()
    return _lora_llm_service


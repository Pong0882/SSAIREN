import os
from faster_whisper import WhisperModel
from typing import Optional, Dict, Any, Generator
import time

# Whisper ëª¨ë¸ ì‹±ê¸€í†¤ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ ìœ„í•´ í•œ ë²ˆë§Œ ë¡œë“œ)
_whisper_model: Optional[WhisperModel] = None

def get_whisper_model() -> WhisperModel:
    """
    Faster-Whisper ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (lazy initialization)

    í™˜ê²½ë³€ìˆ˜:
        WHISPER_MODEL_SIZE: ëª¨ë¸ í¬ê¸° (tiny/base/small/medium/large-v3, ê¸°ë³¸: medium)
        WHISPER_DEVICE: ë””ë°”ì´ìŠ¤ (cpu/cuda, ê¸°ë³¸: cpu)
        WHISPER_COMPUTE_TYPE: ì—°ì‚° íƒ€ì… (int8/float16/float32, ê¸°ë³¸: int8)
    """
    global _whisper_model

    if _whisper_model is None:
        # í™˜ê²½ë³€ìˆ˜ì—ì„œ ì„¤ì • ì½ê¸°
        model_size = os.getenv("WHISPER_MODEL_SIZE", "medium")
        device = os.getenv("WHISPER_DEVICE", "cpu")  # "cpu" or "cuda"
        compute_type = os.getenv("WHISPER_COMPUTE_TYPE", "int8")  # CPU: "int8", GPU: "float16"

        print(f"ğŸ”„ Loading Whisper model: {model_size} on {device} with {compute_type}")
        start_time = time.time()

        _whisper_model = WhisperModel(
            model_size,
            device=device,
            compute_type=compute_type,
            download_root="./models/whisper"  # ëª¨ë¸ ì €ì¥ ìœ„ì¹˜
        )

        load_time = time.time() - start_time
        print(f"âœ… Whisper model loaded in {load_time:.2f}s")

    return _whisper_model


def transcribe_audio_stream(
    audio_file_path: str,
    language: str = "ko"
) -> Generator[Dict[str, Any], None, None]:
    """
    Faster-Whisperë¥¼ ì‚¬ìš©í•œ ìŒì„±-í…ìŠ¤íŠ¸ ë³€í™˜ (ì„¸ê·¸ë¨¼íŠ¸ ìŠ¤íŠ¸ë¦¬ë°)

    Args:
        audio_file_path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ (.mp3, .wav, .m4a ë“±)
        language: ì–¸ì–´ ì½”ë“œ (ê¸°ë³¸ê°’: "ko")

    Yields:
        ì„¸ê·¸ë¨¼íŠ¸ ì •ë³´ë¥¼ í¬í•¨í•œ ë”•ì…”ë„ˆë¦¬
        {
            "start": float,  # ì‹œì‘ ì‹œê°„ (ì´ˆ)
            "end": float,    # ì¢…ë£Œ ì‹œê°„ (ì´ˆ)
            "text": str,     # ë³€í™˜ëœ í…ìŠ¤íŠ¸
            "avg_logprob": float,      # í‰ê·  ë¡œê·¸ í™•ë¥ 
            "no_speech_prob": float    # ë¬´ìŒ í™•ë¥ 
        }
    """
    model = get_whisper_model()

    print(f"ğŸ¤ Transcribing: {audio_file_path}")
    start_time = time.time()

    # Whisper ì¶”ë¡  ìˆ˜í–‰ (ì •í™•ë„ ìµœìš°ì„  ì„¤ì •)
    segments, info = model.transcribe(
        audio_file_path,
        language=language,
        # ì •í™•ë„ í–¥ìƒ íŒŒë¼ë¯¸í„°
        beam_size=10,  # ë¹” ì„œì¹˜ í¬ê¸° (5â†’10, ëŠë¦¬ì§€ë§Œ ì •í™•í•¨)
        best_of=5,  # ìƒìœ„ 5ê°œ í›„ë³´ ì¤‘ ìµœì„  ì„ íƒ
        temperature=0.0,  # ê²°ì •ì  ì¶œë ¥ (0.0 = ê°€ì¥ ì •í™•)
        condition_on_previous_text=True,  # ì´ì „ ì»¨í…ìŠ¤íŠ¸ í™œìš©
        # ìŒì„± í’ˆì§ˆ í–¥ìƒ
        vad_filter=True,  # Voice Activity Detection (ë¬´ìŒ ì œê±°)
        vad_parameters=dict(
            min_silence_duration_ms=500,  # 500ms ì´ìƒ ë¬´ìŒ ì œê±°
            threshold=0.5  # VAD ì„ê³„ê°’
        )
    )

    print(f"ğŸŒ Detected language: {info.language} (probability: {info.language_probability:.2f})")

    segment_count = 0
    # ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ìˆœíšŒí•˜ë©° yield
    for segment in segments:
        segment_count += 1
        yield {
            "start": round(segment.start, 2),
            "end": round(segment.end, 2),
            "text": segment.text.strip(),
            "avg_logprob": round(segment.avg_logprob, 3),
            "no_speech_prob": round(segment.no_speech_prob, 3)
        }

    total_time = time.time() - start_time
    print(f"âœ… Transcription completed: {segment_count} segments in {total_time:.2f}s")


def transcribe_audio_full(audio_file_path: str, language: str = "ko") -> Dict[str, Any]:
    """
    Faster-Whisperë¥¼ ì‚¬ìš©í•œ ìŒì„±-í…ìŠ¤íŠ¸ ë³€í™˜ (ì „ì²´ í…ìŠ¤íŠ¸ ë°˜í™˜)

    Args:
        audio_file_path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        language: ì–¸ì–´ ì½”ë“œ (ê¸°ë³¸ê°’: "ko")

    Returns:
        {
            "text": str,           # ì „ì²´ ë³€í™˜ëœ í…ìŠ¤íŠ¸
            "segments": list,      # ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸
            "language": str,       # ê°ì§€ëœ ì–¸ì–´
            "duration": float      # ì²˜ë¦¬ ì‹œê°„ (ì´ˆ)
        }
    """
    start_time = time.time()
    segments = list(transcribe_audio_stream(audio_file_path, language))
    duration = time.time() - start_time

    full_text = " ".join([seg["text"] for seg in segments])

    return {
        "text": full_text,
        "segments": segments,
        "language": language,
        "duration": round(duration, 2)
    }
